load config from['./Accuracy/config/transformer/config_stt2_bert_ibert_infer.ini']
===============================configurations===============================
Section Network   
	 model                : BERT
	 dataset              : GLUE
	 data_path            : /home/wangcong/dataset/glue_data
	 task                 : SST-2
	 pretrained           : True
	 model_path           : /home/wangcong/pretrain/huggingface/hub/models--bert-base-uncased/snapshots/models
Section Quantization
	 mode                 : IBERT
	 embedding            : True
	 weightprecision      : 8
	 inputprecision       : 8
	 biasprecision        : 32
	 errorprecision       : -1
	 gradientprecision    : -1
	 weightsignmapping    : TwosComp
	 inputsignmapping     : TwosComp
	 weightmapping        : Sign
	 inputmapping         : Sign
	 hardware             : True
	 dumpaveragevalue     : False
	 dumpaveragevalue_path : ./bert_avg/STT2/CASE2_RRAM1/
Section Training  
	 batch_size           : 32
	 max_seq_length       : 128
	 learning_rate        : 2e-5
	 bn_learning_rate     : 0.1
	 numepoch             : 5
	 decreasing_lr        : 1
	 train_log_interval   : 100
	 val_log_interval     : 1
Section Inference 
	 finetuned            : True
	 finetuned_model      : /home/wangcong/pretrain/huggingface/hub/models--bert-base-uncased/snapshots/models/SST-2/IBERT
Section Path      
	 log_dir              : ./log/transformer
	 organize             : Network_model,Network_dataset,Network_task,Quantization_mode
	 tag                  : datadump
Section Debug     
	 printlinearerr       : False
	 printmatmulerr       : False
Section CIM       
	 arraysize            : 128
	 cellprecision        : 1
	 cycleprecision       : 1
	 digitref2            : False
	 digitref3            : False
	 withcellvar          : False
Section Device    
	 resmap               : ./Accuracy/src/Component/cell_files/fake_device_ronoff.csv
	 gmincancel           : False
Section ADC       
	 mode                 : Linear
	 type                 : SAR
	 share                : 8
	 dumpdata             : False
	 dumpdatapath         : dumped_data
	 std_file             : ./Accuracy/src/Component/ADC_files/Linear_std/test.csv
	 ref_file             : ./Accuracy/src/Component/ADC_files/NLinear_levels/test.csv
	 linear_file          : ./Accuracy/src/Component/ADC_files/Linear/BERT/IBERT/w1in1/Case1/bit6.csv
	 nlinear_file         : ./Accuracy/src/Component/ADC_files/NLinear_levels/test.csv
	 nlineartype          : KMEANS
Section DMVMCIM   
	 mixedsignal          : False
Section DMVMKQCIM 
	 arraysize            : 64
	 cellprecision        : 1
	 cycleprecision       : 1
	 digitref2            : True
	 digitref3            : True
	 withcellvar          : False
Section DMVMKQDevice
	 resmap               : ./Accuracy/src/Component/cell_files/fake_device_ronoff.csv
	 gmincancel           : False
Section DMVMKQADC 
	 mode                 : Linear
	 type                 : SAR
	 share                : 8
	 dumpdata             : False
	 dumpdatapath         : dumped_data
	 std_file             : ./Accuracy/src/Component/ADC_files/Linear_std/test.csv
	 ref_file             : ./Accuracy/src/Component/ADC_files/NLinear_levels/test.csv
	 linear_file          : ./Accuracy/src/Component/ADC_files/Linear/BERT/w2in1/Case2/bit7.csv
	 nlinear_file         : ./Accuracy/src/Component/ADC_files/NLinear_levels/test.csv
	 nlineartype          : KMEANS
Section DMVMPVCIM 
	 arraysize            : 128
	 cellprecision        : 1
	 cycleprecision       : 1
	 digitref2            : True
	 digitref3            : True
	 withcellvar          : False
Section DMVMPVDevice
	 resmap               : ./Accuracy/src/Component/cell_files/fake_device_ronoff.csv
	 gmincancel           : False
Section DMVMPVADC 
	 mode                 : Linear
	 type                 : SAR
	 share                : 8
	 dumpdata             : False
	 dumpdatapath         : dumped_data
	 std_file             : ./Accuracy/src/Component/ADC_files/Linear_std/test.csv
	 ref_file             : ./Accuracy/src/Component/ADC_files/NLinear_levels/test.csv
	 linear_file          : ./Accuracy/src/Component/ADC_files/Linear/BERT/w2in1/Case2/bit7.csv
	 nlinear_file         : ./Accuracy/src/Component/ADC_files/NLinear_levels/test.csv
	 nlineartype          : KMEANS
Section NonIdeal  
	 noiseloc             : None
	 noisetype            : Gaussian
	 noisestd             : 10
	 printstat            : False
	 weightnoise          : 0.0
===============================configurations===============================
BertForSequenceClassification(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): QEmbedding(
        (Quantizer): IBERTQuantizer()
      )
      (token_type_embeddings): QEmbedding(
        (Quantizer): IBERTQuantizer()
      )
      (position_embeddings): QEmbedding(
        (Quantizer): IBERTQuantizer()
      )
      (embeddings_act1): QuantAct(activation_bit=16, quant_mode: True, Act_min: -1.50, Act_max: 1.04)
      (embeddings_act2): QuantAct(activation_bit=16, quant_mode: True, Act_min: -0.00, Act_max: 0.00)
      (LayerNorm): IntLayerNorm(
        (activation): QuantAct(activation_bit=32, quant_mode: True, Act_min: -0.00, Act_max: 0.00)
      )
      (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -10.42, Act_max: 5.25)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (key): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (value): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (query_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -8.07, Act_max: 6.58)
              (key_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -7.07, Act_max: 8.88)
              (value_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -4.27, Act_max: 3.25)
              (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -2.11, Act_max: 2.26)
              (softmax): IntSoftmax(
                (act): QuantAct(activation_bit=16, quant_mode: True, Act_min: 9062972.00, Act_max: 9731292086140928.00)
              )
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (ln_input_act): QuantAct(activation_bit=22, quant_mode: True, Act_min: -14.35, Act_max: 6.13)
              (LayerNorm): IntLayerNorm(
                (activation): QuantAct(activation_bit=32, quant_mode: True, Act_min: -0.00, Act_max: 0.00)
              )
              (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -63.38, Act_max: 10.63)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(
              in_features=768, out_features=3072 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
              (quantizer): IBERTQuantizer()
            )
            (intermediate_act_fn): IntGELU()
            (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -0.17, Act_max: 11.13)
          )
          (output): BertOutput(
            (dense): QLinear(
              in_features=3072, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
              (quantizer): IBERTQuantizer()
            )
            (ln_input_act): QuantAct(activation_bit=22, quant_mode: True, Act_min: -67.97, Act_max: 15.42)
            (LayerNorm): IntLayerNorm(
              (activation): QuantAct(activation_bit=32, quant_mode: True, Act_min: -0.00, Act_max: 0.00)
            )
            (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -9.99, Act_max: 3.88)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (pre_intermediate_act): QuantAct(activation_bit=8, quant_mode: True, Act_min: -62.95, Act_max: 10.65)
          (pre_output_act): QuantAct(activation_bit=8, quant_mode: True, Act_min: -0.18, Act_max: 10.90)
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (key): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (value): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (query_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -6.75, Act_max: 6.98)
              (key_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -9.23, Act_max: 7.33)
              (value_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -4.18, Act_max: 4.00)
              (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -2.17, Act_max: 2.33)
              (softmax): IntSoftmax(
                (act): QuantAct(activation_bit=16, quant_mode: True, Act_min: 11069489.00, Act_max: 11885773309607936.00)
              )
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (ln_input_act): QuantAct(activation_bit=22, quant_mode: True, Act_min: -10.96, Act_max: 3.85)
              (LayerNorm): IntLayerNorm(
                (activation): QuantAct(activation_bit=32, quant_mode: True, Act_min: -0.00, Act_max: 0.00)
              )
              (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -70.98, Act_max: 5.21)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(
              in_features=768, out_features=3072 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
              (quantizer): IBERTQuantizer()
            )
            (intermediate_act_fn): IntGELU()
            (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -0.17, Act_max: 10.54)
          )
          (output): BertOutput(
            (dense): QLinear(
              in_features=3072, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
              (quantizer): IBERTQuantizer()
            )
            (ln_input_act): QuantAct(activation_bit=22, quant_mode: True, Act_min: -93.24, Act_max: 9.10)
            (LayerNorm): IntLayerNorm(
              (activation): QuantAct(activation_bit=32, quant_mode: True, Act_min: -0.00, Act_max: 0.00)
            )
            (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -11.41, Act_max: 3.79)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (pre_intermediate_act): QuantAct(activation_bit=8, quant_mode: True, Act_min: -70.96, Act_max: 5.21)
          (pre_output_act): QuantAct(activation_bit=8, quant_mode: True, Act_min: -0.17, Act_max: 10.41)
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (key): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (value): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (query_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -11.88, Act_max: 11.04)
              (key_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -9.75, Act_max: 10.17)
              (value_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -3.70, Act_max: 4.18)
              (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -1.70, Act_max: 1.99)
              (softmax): IntSoftmax(
                (act): QuantAct(activation_bit=16, quant_mode: True, Act_min: 3120142.00, Act_max: 3350610019614720.00)
              )
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (ln_input_act): QuantAct(activation_bit=22, quant_mode: True, Act_min: -11.46, Act_max: 3.95)
              (LayerNorm): IntLayerNorm(
                (activation): QuantAct(activation_bit=32, quant_mode: True, Act_min: -0.00, Act_max: 0.00)
              )
              (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -57.20, Act_max: 5.55)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(
              in_features=768, out_features=3072 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
              (quantizer): IBERTQuantizer()
            )
            (intermediate_act_fn): IntGELU()
            (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -0.17, Act_max: 58.01)
          )
          (output): BertOutput(
            (dense): QLinear(
              in_features=3072, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
              (quantizer): IBERTQuantizer()
            )
            (ln_input_act): QuantAct(activation_bit=22, quant_mode: True, Act_min: -263.95, Act_max: 7.65)
            (LayerNorm): IntLayerNorm(
              (activation): QuantAct(activation_bit=32, quant_mode: True, Act_min: -0.00, Act_max: 0.00)
            )
            (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -14.71, Act_max: 3.53)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (pre_intermediate_act): QuantAct(activation_bit=8, quant_mode: True, Act_min: -57.18, Act_max: 5.55)
          (pre_output_act): QuantAct(activation_bit=8, quant_mode: True, Act_min: -0.00, Act_max: 56.98)
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (key): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (value): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (query_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -7.38, Act_max: 6.81)
              (key_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -5.65, Act_max: 6.61)
              (value_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -4.47, Act_max: 4.88)
              (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -1.74, Act_max: 2.17)
              (softmax): IntSoftmax(
                (act): QuantAct(activation_bit=16, quant_mode: True, Act_min: 18731184.00, Act_max: 20112455673839616.00)
              )
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (ln_input_act): QuantAct(activation_bit=22, quant_mode: True, Act_min: -14.83, Act_max: 3.63)
              (LayerNorm): IntLayerNorm(
                (activation): QuantAct(activation_bit=32, quant_mode: True, Act_min: -0.00, Act_max: 0.00)
              )
              (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -94.48, Act_max: 4.85)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(
              in_features=768, out_features=3072 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
              (quantizer): IBERTQuantizer()
            )
            (intermediate_act_fn): IntGELU()
            (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -0.96, Act_max: 38.41)
          )
          (output): BertOutput(
            (dense): QLinear(
              in_features=3072, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
              (quantizer): IBERTQuantizer()
            )
            (ln_input_act): QuantAct(activation_bit=22, quant_mode: True, Act_min: -440.97, Act_max: 6.69)
            (LayerNorm): IntLayerNorm(
              (activation): QuantAct(activation_bit=32, quant_mode: True, Act_min: -0.00, Act_max: 0.00)
            )
            (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -14.19, Act_max: 3.67)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (pre_intermediate_act): QuantAct(activation_bit=8, quant_mode: True, Act_min: -94.51, Act_max: 4.84)
          (pre_output_act): QuantAct(activation_bit=8, quant_mode: True, Act_min: -0.94, Act_max: 37.15)
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (key): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (value): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (query_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -7.69, Act_max: 6.84)
              (key_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -6.40, Act_max: 5.79)
              (value_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -3.81, Act_max: 4.02)
              (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -1.80, Act_max: 2.09)
              (softmax): IntSoftmax(
                (act): QuantAct(activation_bit=16, quant_mode: True, Act_min: 19119312.00, Act_max: 20529204940505088.00)
              )
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (ln_input_act): QuantAct(activation_bit=22, quant_mode: True, Act_min: -14.81, Act_max: 3.80)
              (LayerNorm): IntLayerNorm(
                (activation): QuantAct(activation_bit=32, quant_mode: True, Act_min: -0.00, Act_max: 0.00)
              )
              (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -102.63, Act_max: 8.11)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(
              in_features=768, out_features=3072 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
              (quantizer): IBERTQuantizer()
            )
            (intermediate_act_fn): IntGELU()
            (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -1.04, Act_max: 18.28)
          )
          (output): BertOutput(
            (dense): QLinear(
              in_features=3072, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
              (quantizer): IBERTQuantizer()
            )
            (ln_input_act): QuantAct(activation_bit=22, quant_mode: True, Act_min: -207.47, Act_max: 17.94)
            (LayerNorm): IntLayerNorm(
              (activation): QuantAct(activation_bit=32, quant_mode: True, Act_min: -0.00, Act_max: 0.00)
            )
            (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -15.12, Act_max: 4.65)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (pre_intermediate_act): QuantAct(activation_bit=8, quant_mode: True, Act_min: -102.61, Act_max: 8.13)
          (pre_output_act): QuantAct(activation_bit=8, quant_mode: True, Act_min: -1.04, Act_max: 14.49)
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (key): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (value): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (query_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -7.08, Act_max: 6.77)
              (key_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -7.06, Act_max: 8.33)
              (value_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -3.74, Act_max: 3.90)
              (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -2.05, Act_max: 2.03)
              (softmax): IntSoftmax(
                (act): QuantAct(activation_bit=16, quant_mode: True, Act_min: 13231242.00, Act_max: 14206937918865408.00)
              )
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (ln_input_act): QuantAct(activation_bit=22, quant_mode: True, Act_min: -16.67, Act_max: 4.38)
              (LayerNorm): IntLayerNorm(
                (activation): QuantAct(activation_bit=32, quant_mode: True, Act_min: -0.00, Act_max: 0.00)
              )
              (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -94.18, Act_max: 17.38)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(
              in_features=768, out_features=3072 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
              (quantizer): IBERTQuantizer()
            )
            (intermediate_act_fn): IntGELU()
            (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -0.55, Act_max: 10.95)
          )
          (output): BertOutput(
            (dense): QLinear(
              in_features=3072, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
              (quantizer): IBERTQuantizer()
            )
            (ln_input_act): QuantAct(activation_bit=22, quant_mode: True, Act_min: -162.50, Act_max: 59.32)
            (LayerNorm): IntLayerNorm(
              (activation): QuantAct(activation_bit=32, quant_mode: True, Act_min: -0.00, Act_max: 0.00)
            )
            (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -16.03, Act_max: 7.92)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (pre_intermediate_act): QuantAct(activation_bit=8, quant_mode: True, Act_min: -94.19, Act_max: 17.34)
          (pre_output_act): QuantAct(activation_bit=8, quant_mode: True, Act_min: -0.53, Act_max: 10.40)
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (key): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (value): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (query_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -7.47, Act_max: 7.01)
              (key_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -7.68, Act_max: 8.33)
              (value_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -3.90, Act_max: 3.99)
              (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -2.52, Act_max: 2.55)
              (softmax): IntSoftmax(
                (act): QuantAct(activation_bit=16, quant_mode: True, Act_min: 11973582.00, Act_max: 12856535776493568.00)
              )
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (ln_input_act): QuantAct(activation_bit=22, quant_mode: True, Act_min: -15.96, Act_max: 7.78)
              (LayerNorm): IntLayerNorm(
                (activation): QuantAct(activation_bit=32, quant_mode: True, Act_min: -0.00, Act_max: 0.00)
              )
              (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -91.39, Act_max: 34.27)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(
              in_features=768, out_features=3072 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
              (quantizer): IBERTQuantizer()
            )
            (intermediate_act_fn): IntGELU()
            (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -0.17, Act_max: 13.19)
          )
          (output): BertOutput(
            (dense): QLinear(
              in_features=3072, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
              (quantizer): IBERTQuantizer()
            )
            (ln_input_act): QuantAct(activation_bit=22, quant_mode: True, Act_min: -135.34, Act_max: 31.59)
            (LayerNorm): IntLayerNorm(
              (activation): QuantAct(activation_bit=32, quant_mode: True, Act_min: -0.00, Act_max: 0.00)
            )
            (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -15.95, Act_max: 5.15)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (pre_intermediate_act): QuantAct(activation_bit=8, quant_mode: True, Act_min: -91.27, Act_max: 34.24)
          (pre_output_act): QuantAct(activation_bit=8, quant_mode: True, Act_min: -0.21, Act_max: 13.01)
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (key): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (value): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (query_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -7.39, Act_max: 6.74)
              (key_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -6.44, Act_max: 6.64)
              (value_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -5.19, Act_max: 5.76)
              (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -2.51, Act_max: 2.53)
              (softmax): IntSoftmax(
                (act): QuantAct(activation_bit=16, quant_mode: True, Act_min: 19322956.00, Act_max: 20747866020511744.00)
              )
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (ln_input_act): QuantAct(activation_bit=22, quant_mode: True, Act_min: -16.22, Act_max: 5.48)
              (LayerNorm): IntLayerNorm(
                (activation): QuantAct(activation_bit=32, quant_mode: True, Act_min: -0.00, Act_max: 0.00)
              )
              (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -90.45, Act_max: 18.67)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(
              in_features=768, out_features=3072 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
              (quantizer): IBERTQuantizer()
            )
            (intermediate_act_fn): IntGELU()
            (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -0.17, Act_max: 7.50)
          )
          (output): BertOutput(
            (dense): QLinear(
              in_features=3072, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
              (quantizer): IBERTQuantizer()
            )
            (ln_input_act): QuantAct(activation_bit=22, quant_mode: True, Act_min: -115.55, Act_max: 16.18)
            (LayerNorm): IntLayerNorm(
              (activation): QuantAct(activation_bit=32, quant_mode: True, Act_min: -0.00, Act_max: 0.00)
            )
            (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -19.70, Act_max: 4.12)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (pre_intermediate_act): QuantAct(activation_bit=8, quant_mode: True, Act_min: -90.38, Act_max: 18.63)
          (pre_output_act): QuantAct(activation_bit=8, quant_mode: True, Act_min: -0.18, Act_max: 7.39)
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (key): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (value): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (query_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -6.79, Act_max: 6.48)
              (key_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -5.78, Act_max: 6.18)
              (value_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -5.72, Act_max: 5.32)
              (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -3.05, Act_max: 3.06)
              (softmax): IntSoftmax(
                (act): QuantAct(activation_bit=16, quant_mode: True, Act_min: 25939852.00, Act_max: 27852704000770048.00)
              )
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (ln_input_act): QuantAct(activation_bit=22, quant_mode: True, Act_min: -20.70, Act_max: 4.98)
              (LayerNorm): IntLayerNorm(
                (activation): QuantAct(activation_bit=32, quant_mode: True, Act_min: -0.00, Act_max: 0.00)
              )
              (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -87.02, Act_max: 12.39)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(
              in_features=768, out_features=3072 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
              (quantizer): IBERTQuantizer()
            )
            (intermediate_act_fn): IntGELU()
            (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -0.18, Act_max: 13.08)
          )
          (output): BertOutput(
            (dense): QLinear(
              in_features=3072, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
              (quantizer): IBERTQuantizer()
            )
            (ln_input_act): QuantAct(activation_bit=22, quant_mode: True, Act_min: -136.40, Act_max: 31.44)
            (LayerNorm): IntLayerNorm(
              (activation): QuantAct(activation_bit=32, quant_mode: True, Act_min: -0.00, Act_max: 0.00)
            )
            (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -22.09, Act_max: 5.90)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (pre_intermediate_act): QuantAct(activation_bit=8, quant_mode: True, Act_min: -86.86, Act_max: 12.45)
          (pre_output_act): QuantAct(activation_bit=8, quant_mode: True, Act_min: -0.20, Act_max: 11.81)
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (key): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (value): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (query_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -7.79, Act_max: 7.26)
              (key_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -6.18, Act_max: 7.02)
              (value_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -8.15, Act_max: 6.05)
              (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -5.46, Act_max: 3.93)
              (softmax): IntSoftmax(
                (act): QuantAct(activation_bit=16, quant_mode: True, Act_min: 15787380.00, Act_max: 16951570197381120.00)
              )
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (ln_input_act): QuantAct(activation_bit=22, quant_mode: True, Act_min: -22.57, Act_max: 6.49)
              (LayerNorm): IntLayerNorm(
                (activation): QuantAct(activation_bit=32, quant_mode: True, Act_min: -0.00, Act_max: 0.00)
              )
              (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -37.54, Act_max: 13.49)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(
              in_features=768, out_features=3072 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
              (quantizer): IBERTQuantizer()
            )
            (intermediate_act_fn): IntGELU()
            (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -0.18, Act_max: 52.45)
          )
          (output): BertOutput(
            (dense): QLinear(
              in_features=3072, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
              (quantizer): IBERTQuantizer()
            )
            (ln_input_act): QuantAct(activation_bit=22, quant_mode: True, Act_min: -426.45, Act_max: 294.04)
            (LayerNorm): IntLayerNorm(
              (activation): QuantAct(activation_bit=32, quant_mode: True, Act_min: -0.00, Act_max: 0.00)
            )
            (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -24.02, Act_max: 3.82)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (pre_intermediate_act): QuantAct(activation_bit=8, quant_mode: True, Act_min: -36.84, Act_max: 13.51)
          (pre_output_act): QuantAct(activation_bit=8, quant_mode: True, Act_min: -0.00, Act_max: 49.97)
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (key): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (value): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (query_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -7.29, Act_max: 7.62)
              (key_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -6.89, Act_max: 6.86)
              (value_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -5.01, Act_max: 6.70)
              (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -2.92, Act_max: 4.07)
              (softmax): IntSoftmax(
                (act): QuantAct(activation_bit=16, quant_mode: True, Act_min: 17133480.00, Act_max: 18396934066667520.00)
              )
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (ln_input_act): QuantAct(activation_bit=22, quant_mode: True, Act_min: -22.82, Act_max: 9.55)
              (LayerNorm): IntLayerNorm(
                (activation): QuantAct(activation_bit=32, quant_mode: True, Act_min: -0.00, Act_max: 0.00)
              )
              (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -31.76, Act_max: 13.91)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(
              in_features=768, out_features=3072 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
              (quantizer): IBERTQuantizer()
            )
            (intermediate_act_fn): IntGELU()
            (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -0.43, Act_max: 95.86)
          )
          (output): BertOutput(
            (dense): QLinear(
              in_features=3072, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
              (quantizer): IBERTQuantizer()
            )
            (ln_input_act): QuantAct(activation_bit=22, quant_mode: True, Act_min: -783.98, Act_max: 31.46)
            (LayerNorm): IntLayerNorm(
              (activation): QuantAct(activation_bit=32, quant_mode: True, Act_min: -0.00, Act_max: 0.00)
            )
            (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -27.55, Act_max: 3.97)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (pre_intermediate_act): QuantAct(activation_bit=8, quant_mode: True, Act_min: -31.28, Act_max: 13.89)
          (pre_output_act): QuantAct(activation_bit=8, quant_mode: True, Act_min: -0.74, Act_max: 91.87)
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (key): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (value): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (query_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -6.30, Act_max: 6.95)
              (key_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -7.41, Act_max: 7.03)
              (value_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -6.19, Act_max: 5.83)
              (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -4.08, Act_max: 3.84)
              (softmax): IntSoftmax(
                (act): QuantAct(activation_bit=16, quant_mode: True, Act_min: 17245196.00, Act_max: 18516888208277504.00)
              )
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (ln_input_act): QuantAct(activation_bit=22, quant_mode: True, Act_min: -27.90, Act_max: 5.78)
              (LayerNorm): IntLayerNorm(
                (activation): QuantAct(activation_bit=32, quant_mode: True, Act_min: -0.00, Act_max: 0.00)
              )
              (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -37.13, Act_max: 6.43)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(
              in_features=768, out_features=3072 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
              (quantizer): IBERTQuantizer()
            )
            (intermediate_act_fn): IntGELU()
            (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -0.17, Act_max: 7.47)
          )
          (output): BertOutput(
            (dense): QLinear(
              in_features=3072, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
              (quantizer): IBERTQuantizer()
            )
            (ln_input_act): QuantAct(activation_bit=22, quant_mode: True, Act_min: -36.24, Act_max: 10.46)
            (LayerNorm): IntLayerNorm(
              (activation): QuantAct(activation_bit=32, quant_mode: True, Act_min: -0.00, Act_max: 0.00)
            )
            (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -9.99, Act_max: 3.83)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (pre_intermediate_act): QuantAct(activation_bit=8, quant_mode: True, Act_min: -36.91, Act_max: 6.40)
          (pre_output_act): QuantAct(activation_bit=8, quant_mode: True, Act_min: -0.18, Act_max: 7.22)
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=2, bias=True)
)
start time: 2024_07_16_21_14_50
===================== testing phase =====================
Test result: Acc: 89.5642% F1 score: 0.900110
finish time: 2024_07_16_21_18_33
