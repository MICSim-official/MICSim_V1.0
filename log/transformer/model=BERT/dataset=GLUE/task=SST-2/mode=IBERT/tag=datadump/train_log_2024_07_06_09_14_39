load config from['./Accuracy/config/transformer/config_stt2_bert(large)_ibert_train.ini']
===============================configurations===============================
Section Network   
	 model                : BERT
	 dataset              : GLUE
	 data_path            : /home/wangcong/dataset/glue_data
	 task                 : SST-2
	 pretrained           : True
	 model_path           : /home/wangcong/pretrain/huggingface/hub/models--bert-large-uncased/snapshots/models
Section Quantization
	 mode                 : IBERT
	 embedding            : True
	 weightprecision      : 8
	 inputprecision       : 8
	 biasprecision        : 32
	 errorprecision       : -1
	 gradientprecision    : -1
	 weightsignmapping    : TwosComp
	 inputsignmapping     : TwosComp
	 weightmapping        : Sign
	 inputmapping         : Sign
	 hardware             : False
	 dumpaveragevalue     : False
	 dumpaveragevalue_path : ./log/transformer/SST2/average/
Section Training  
	 batch_size           : 16
	 max_seq_length       : 128
	 learning_rate        : 2e-5
	 bn_learning_rate     : 0.1
	 numepoch             : 5
	 decreasing_lr        : 3, 5
	 train_log_interval   : 100
	 val_log_interval     : 1
Section Inference 
	 finetuned            : True
	 finetuned_model      : /home/wangcong/pretrain/huggingface/hub/models--bert-large-uncased/snapshots/models/SST-2/FloatingPoint
Section Path      
	 log_dir              : ./log/transformer
	 organize             : Network_model,Network_dataset,Network_task,Quantization_mode
	 tag                  : datadump
Section CIM       
	 arraysize            : 128
	 cellprecision        : 1
	 cycleprecision       : 1
	 digitref2            : True
	 digitref3            : True
	 withcellvar          : False
Section Device    
	 resmap               : ./cimsim/Component/cell_files/fake_device_test.csv
	 gmincancel           : False
Section Debug     
	 printlinearerr       : False
	 printmatmulerr       : False
Section ADC       
	 mode                 : Linear
	 type                 : SAR
	 share                : 8
	 dumpdata             : False
	 dumpdatapath         : dumped_data
	 std_file             : ./ADC_files/Linear_std/test.csv
	 ref_file             : .ADC_files/NLinear_levels/test.csv
	 linear_file          : ./ADC_files/Linear/VGG8/basic/bit9.csv
	 nlinear_file         : ./ADC_files/NLinear_levels/test.csv
	 nlineartype          : KMEANS
Section DMVMCIM   
	 mixedsignal          : True
	 arraysize            : 128
	 cellprecision        : 1
	 cycleprecision       : 1
	 digitref2            : True
	 digitref3            : True
	 withcellvar          : False
Section DMVMDevice
	 resmap               : ./cimsim/Component/cell_files/fake_device_test.csv
	 gmincancel           : False
Section DMVMADC   
	 mode                 : Linear
	 type                 : SAR
	 share                : 8
	 dumpdata             : False
	 dumpdatapath         : dumped_data
	 std_file             : ./cimsim/Component/ADC_files/Linear_std/test.csv
	 ref_file             : ./cimsim/Component/ADC_files/NLinear_levels/test.csv
	 linear_file          : ./cimsim/Component/ADC_files/Linear/BERT/bit7.csv
	 nlinear_file         : ./cimsim/Component/ADC_files/NLinear_levels/test.csv
	 nlineartype          : KMEANS
Section NonIdeal  
	 noiseloc             : None
	 noisetype            : Gaussian
	 noisestd             : 10
	 printstat            : False
	 weightnoise          : 0.0
===============================configurations===============================
BertForSequenceClassification(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): QEmbedding(
        (Quantizer): IBERTQuantizer()
      )
      (token_type_embeddings): QEmbedding(
        (Quantizer): IBERTQuantizer()
      )
      (position_embeddings): QEmbedding(
        (Quantizer): IBERTQuantizer()
      )
      (embeddings_act1): QuantAct(activation_bit=16, quant_mode: True, Act_min: -0.00, Act_max: 0.00)
      (embeddings_act2): QuantAct(activation_bit=16, quant_mode: True, Act_min: -0.00, Act_max: 0.00)
      (LayerNorm): IntLayerNorm(
        (activation): QuantAct(activation_bit=32, quant_mode: True, Act_min: -0.00, Act_max: 0.00)
      )
      (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -0.00, Act_max: 0.00)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-23): 24 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): QLinear(
                in_features=1024, out_features=1024 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (key): QLinear(
                in_features=1024, out_features=1024 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (value): QLinear(
                in_features=1024, out_features=1024 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (query_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -0.00, Act_max: 0.00)
              (key_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -0.00, Act_max: 0.00)
              (value_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -0.00, Act_max: 0.00)
              (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -0.00, Act_max: 0.00)
              (softmax): IntSoftmax(
                (act): QuantAct(activation_bit=16, quant_mode: True, Act_min: -0.00, Act_max: 0.00)
              )
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(
                in_features=1024, out_features=1024 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): IBERTQuantizer()
              )
              (ln_input_act): QuantAct(activation_bit=22, quant_mode: True, Act_min: -0.00, Act_max: 0.00)
              (LayerNorm): IntLayerNorm(
                (activation): QuantAct(activation_bit=32, quant_mode: True, Act_min: -0.00, Act_max: 0.00)
              )
              (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -0.00, Act_max: 0.00)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(
              in_features=1024, out_features=4096 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
              (quantizer): IBERTQuantizer()
            )
            (intermediate_act_fn): IntGELU()
            (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -0.00, Act_max: 0.00)
          )
          (output): BertOutput(
            (dense): QLinear(
              in_features=4096, out_features=1024 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
              (quantizer): IBERTQuantizer()
            )
            (ln_input_act): QuantAct(activation_bit=22, quant_mode: True, Act_min: -0.00, Act_max: 0.00)
            (LayerNorm): IntLayerNorm(
              (activation): QuantAct(activation_bit=32, quant_mode: True, Act_min: -0.00, Act_max: 0.00)
            )
            (output_activation): QuantAct(activation_bit=8, quant_mode: True, Act_min: -0.00, Act_max: 0.00)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (pre_intermediate_act): QuantAct(activation_bit=8, quant_mode: True, Act_min: -0.00, Act_max: 0.00)
          (pre_output_act): QuantAct(activation_bit=8, quant_mode: True, Act_min: -0.00, Act_max: 0.00)
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=1024, out_features=1024, bias=True)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=1024, out_features=2, bias=True)
)
Train Epoch: 0 [0/4] Loss: 0.0213 Acc: 0.8125 lr:0.000020
Train Epoch: 0 [0/4] Loss: 0.0210 Acc: 0.8438 lr:0.000020
Train Epoch: 0 [0/4] Loss: 0.0207 Acc: 0.8542 lr:0.000020
Train Epoch: 0 [0/4] Loss: 0.0087 Acc: 0.8750 lr:0.000020
Train Epoch: 0 [0/4] Loss: 0.0048 Acc: 0.9000 lr:0.000020
Train Epoch: 0 [0/4] Loss: 0.0116 Acc: 0.9062 lr:0.000020
Train Epoch: 0 [0/4] Loss: 0.0113 Acc: 0.9107 lr:0.000020
Train Epoch: 0 [0/4] Loss: 0.0224 Acc: 0.8984 lr:0.000020
Train Epoch: 0 [0/4] Loss: 0.0289 Acc: 0.8889 lr:0.000020
Train Epoch: 0 [0/4] Loss: 0.0153 Acc: 0.8875 lr:0.000020
Train Epoch: 0 [0/4] Loss: 0.0315 Acc: 0.8807 lr:0.000020
Train Epoch: 0 [0/4] Loss: 0.0095 Acc: 0.8854 lr:0.000020
Train Epoch: 0 [0/4] Loss: 0.0115 Acc: 0.8894 lr:0.000020
Train Epoch: 0 [0/4] Loss: 0.0036 Acc: 0.8973 lr:0.000020
Train Epoch: 0 [0/4] Loss: 0.0135 Acc: 0.9000 lr:0.000020
Train Epoch: 0 [0/4] Loss: 0.0191 Acc: 0.8945 lr:0.000020
Train Epoch: 0 [0/4] Loss: 0.0133 Acc: 0.8934 lr:0.000020
Train Epoch: 0 [0/4] Loss: 0.0181 Acc: 0.8958 lr:0.000020
Train Epoch: 0 [0/4] Loss: 0.0157 Acc: 0.8947 lr:0.000020
Train Epoch: 0 [0/4] Loss: 0.0117 Acc: 0.8969 lr:0.000020
Train Epoch: 0 [0/4] Loss: 0.0188 Acc: 0.8929 lr:0.000020
Train Epoch: 0 [0/4] Loss: 0.0047 Acc: 0.8949 lr:0.000020
Train Epoch: 0 [0/4] Loss: 0.0140 Acc: 0.8940 lr:0.000020
Train Epoch: 0 [0/4] Loss: 0.0322 Acc: 0.8932 lr:0.000020
Train Epoch: 0 [0/4] Loss: 0.0118 Acc: 0.8925 lr:0.000020
Train Epoch: 0 [0/4] Loss: 0.0103 Acc: 0.8942 lr:0.000020
Train Epoch: 0 [0/4] Loss: 0.0361 Acc: 0.8912 lr:0.000020
Train Epoch: 0 [0/4] Loss: 0.0038 Acc: 0.8951 lr:0.000020
Train Epoch: 0 [0/4] Loss: 0.0065 Acc: 0.8966 lr:0.000020
Train Epoch: 0 [0/4] Loss: 0.0273 Acc: 0.8938 lr:0.000020
Train Epoch: 0 [0/4] Loss: 0.0067 Acc: 0.8952 lr:0.000020
Train Epoch: 0 [0/4] Loss: 0.0053 Acc: 0.8965 lr:0.000020
Train Epoch: 0 [0/4] Loss: 0.0087 Acc: 0.8977 lr:0.000020
Train Epoch: 0 [0/4] Loss: 0.0047 Acc: 0.9007 lr:0.000020
Train Epoch: 0 [0/4] Loss: 0.0151 Acc: 0.9018 lr:0.000020
Train Epoch: 0 [0/4] Loss: 0.0047 Acc: 0.9045 lr:0.000020
Train Epoch: 0 [0/4] Loss: 0.0208 Acc: 0.9020 lr:0.000020
Train Epoch: 0 [0/4] Loss: 0.0165 Acc: 0.8980 lr:0.000020
Train Epoch: 0 [0/4] Loss: 0.0074 Acc: 0.8990 lr:0.000020
Train Epoch: 0 [0/4] Loss: 0.0028 Acc: 0.9016 lr:0.000020
Train Epoch: 0 [0/4] Loss: 0.0039 Acc: 0.9040 lr:0.000020
Train Epoch: 0 [0/4] Loss: 0.0114 Acc: 0.9048 lr:0.000020
Train Epoch: 0 [0/4] Loss: 0.0051 Acc: 0.9070 lr:0.000020
===================== testing phase =====================
Eval Epoch: 0 [0/5] Acc: 0.9048 F1 score: 0.9079
save best performance model to 
/home/wangcong/pretrain/huggingface/hub/models--bert-large-uncased/snapshots/models/SST-2/IBERT/
Train Epoch: 1 [1/4] Loss: 0.0098 Acc: 0.9375 lr:0.000020
Train Epoch: 1 [1/4] Loss: 0.0106 Acc: 0.9375 lr:0.000020
Train Epoch: 1 [1/4] Loss: 0.0163 Acc: 0.9167 lr:0.000020
Train Epoch: 1 [1/4] Loss: 0.0060 Acc: 0.9375 lr:0.000020
Train Epoch: 1 [1/4] Loss: 0.0106 Acc: 0.9375 lr:0.000020
Train Epoch: 1 [1/4] Loss: 0.0066 Acc: 0.9375 lr:0.000020
Train Epoch: 1 [1/4] Loss: 0.0038 Acc: 0.9464 lr:0.000020
Train Epoch: 1 [1/4] Loss: 0.0098 Acc: 0.9453 lr:0.000020
Train Epoch: 1 [1/4] Loss: 0.0169 Acc: 0.9444 lr:0.000020
Train Epoch: 1 [1/4] Loss: 0.0074 Acc: 0.9437 lr:0.000020
Train Epoch: 1 [1/4] Loss: 0.0308 Acc: 0.9318 lr:0.000020
Train Epoch: 1 [1/4] Loss: 0.0031 Acc: 0.9375 lr:0.000020
Train Epoch: 1 [1/4] Loss: 0.0077 Acc: 0.9375 lr:0.000020
Train Epoch: 1 [1/4] Loss: 0.0152 Acc: 0.9375 lr:0.000020
Train Epoch: 1 [1/4] Loss: 0.0119 Acc: 0.9375 lr:0.000020
Train Epoch: 1 [1/4] Loss: 0.0097 Acc: 0.9375 lr:0.000020
Train Epoch: 1 [1/4] Loss: 0.0273 Acc: 0.9301 lr:0.000020
Train Epoch: 1 [1/4] Loss: 0.0304 Acc: 0.9201 lr:0.000020
Train Epoch: 1 [1/4] Loss: 0.0291 Acc: 0.9178 lr:0.000020
Train Epoch: 1 [1/4] Loss: 0.0118 Acc: 0.9156 lr:0.000020
Train Epoch: 1 [1/4] Loss: 0.0237 Acc: 0.9137 lr:0.000020
Train Epoch: 1 [1/4] Loss: 0.0139 Acc: 0.9091 lr:0.000020
Train Epoch: 1 [1/4] Loss: 0.0273 Acc: 0.9076 lr:0.000020
Train Epoch: 1 [1/4] Loss: 0.0310 Acc: 0.8984 lr:0.000020
Train Epoch: 1 [1/4] Loss: 0.0071 Acc: 0.9025 lr:0.000020
Train Epoch: 1 [1/4] Loss: 0.0176 Acc: 0.9038 lr:0.000020
Train Epoch: 1 [1/4] Loss: 0.0229 Acc: 0.9005 lr:0.000020
Train Epoch: 1 [1/4] Loss: 0.0088 Acc: 0.9018 lr:0.000020
Train Epoch: 1 [1/4] Loss: 0.0019 Acc: 0.9052 lr:0.000020
Train Epoch: 1 [1/4] Loss: 0.0399 Acc: 0.9000 lr:0.000020
Train Epoch: 1 [1/4] Loss: 0.0063 Acc: 0.9012 lr:0.000020
Train Epoch: 1 [1/4] Loss: 0.0079 Acc: 0.9023 lr:0.000020
Train Epoch: 1 [1/4] Loss: 0.0139 Acc: 0.8996 lr:0.000020
Train Epoch: 1 [1/4] Loss: 0.0036 Acc: 0.9026 lr:0.000020
Train Epoch: 1 [1/4] Loss: 0.0088 Acc: 0.9036 lr:0.000020
Train Epoch: 1 [1/4] Loss: 0.0049 Acc: 0.9062 lr:0.000020
Train Epoch: 1 [1/4] Loss: 0.0332 Acc: 0.9020 lr:0.000020
Train Epoch: 1 [1/4] Loss: 0.0133 Acc: 0.9013 lr:0.000020
Train Epoch: 1 [1/4] Loss: 0.0138 Acc: 0.9022 lr:0.000020
Train Epoch: 1 [1/4] Loss: 0.0084 Acc: 0.9031 lr:0.000020
Train Epoch: 1 [1/4] Loss: 0.0061 Acc: 0.9040 lr:0.000020
Train Epoch: 1 [1/4] Loss: 0.0271 Acc: 0.9003 lr:0.000020
Train Epoch: 1 [1/4] Loss: 0.0188 Acc: 0.9012 lr:0.000020
===================== testing phase =====================
Eval Epoch: 1 [1/5] Acc: 0.8899 F1 score: 0.8959
Train Epoch: 2 [2/4] Loss: 0.0179 Acc: 0.8750 lr:0.000020
Train Epoch: 2 [2/4] Loss: 0.0245 Acc: 0.8750 lr:0.000020
Train Epoch: 2 [2/4] Loss: 0.0264 Acc: 0.8542 lr:0.000020
Train Epoch: 2 [2/4] Loss: 0.0251 Acc: 0.8594 lr:0.000020
Train Epoch: 2 [2/4] Loss: 0.0052 Acc: 0.8875 lr:0.000020
Train Epoch: 2 [2/4] Loss: 0.0055 Acc: 0.9062 lr:0.000020
Train Epoch: 2 [2/4] Loss: 0.0091 Acc: 0.9107 lr:0.000020
Train Epoch: 2 [2/4] Loss: 0.0140 Acc: 0.9141 lr:0.000020
Train Epoch: 2 [2/4] Loss: 0.0335 Acc: 0.8819 lr:0.000020
Train Epoch: 2 [2/4] Loss: 0.0230 Acc: 0.8750 lr:0.000020
Train Epoch: 2 [2/4] Loss: 0.0399 Acc: 0.8523 lr:0.000020
Train Epoch: 2 [2/4] Loss: 0.0214 Acc: 0.8594 lr:0.000020
Train Epoch: 2 [2/4] Loss: 0.0158 Acc: 0.8606 lr:0.000020
Train Epoch: 2 [2/4] Loss: 0.0352 Acc: 0.8571 lr:0.000020
Train Epoch: 2 [2/4] Loss: 0.0152 Acc: 0.8583 lr:0.000020
Train Epoch: 2 [2/4] Loss: 0.0066 Acc: 0.8672 lr:0.000020
Train Epoch: 2 [2/4] Loss: 0.0343 Acc: 0.8603 lr:0.000020
Train Epoch: 2 [2/4] Loss: 0.0102 Acc: 0.8646 lr:0.000020
Train Epoch: 2 [2/4] Loss: 0.0209 Acc: 0.8684 lr:0.000020
Train Epoch: 2 [2/4] Loss: 0.0174 Acc: 0.8719 lr:0.000020
Train Epoch: 2 [2/4] Loss: 0.0209 Acc: 0.8690 lr:0.000020
Train Epoch: 2 [2/4] Loss: 0.0165 Acc: 0.8722 lr:0.000020
Train Epoch: 2 [2/4] Loss: 0.0231 Acc: 0.8723 lr:0.000020
Train Epoch: 2 [2/4] Loss: 0.0243 Acc: 0.8698 lr:0.000020
Train Epoch: 2 [2/4] Loss: 0.0094 Acc: 0.8725 lr:0.000020
Train Epoch: 2 [2/4] Loss: 0.0095 Acc: 0.8750 lr:0.000020
Train Epoch: 2 [2/4] Loss: 0.0185 Acc: 0.8773 lr:0.000020
Train Epoch: 2 [2/4] Loss: 0.0104 Acc: 0.8772 lr:0.000020
Train Epoch: 2 [2/4] Loss: 0.0025 Acc: 0.8815 lr:0.000020
Train Epoch: 2 [2/4] Loss: 0.0291 Acc: 0.8812 lr:0.000020
Train Epoch: 2 [2/4] Loss: 0.0159 Acc: 0.8831 lr:0.000020
Train Epoch: 2 [2/4] Loss: 0.0032 Acc: 0.8867 lr:0.000020
Train Epoch: 2 [2/4] Loss: 0.0050 Acc: 0.8902 lr:0.000020
Train Epoch: 2 [2/4] Loss: 0.0096 Acc: 0.8934 lr:0.000020
Train Epoch: 2 [2/4] Loss: 0.0090 Acc: 0.8946 lr:0.000020
Train Epoch: 2 [2/4] Loss: 0.0077 Acc: 0.8958 lr:0.000020
Train Epoch: 2 [2/4] Loss: 0.0431 Acc: 0.8936 lr:0.000020
Train Epoch: 2 [2/4] Loss: 0.0326 Acc: 0.8882 lr:0.000020
Train Epoch: 2 [2/4] Loss: 0.0046 Acc: 0.8910 lr:0.000020
Train Epoch: 2 [2/4] Loss: 0.0230 Acc: 0.8891 lr:0.000020
Train Epoch: 2 [2/4] Loss: 0.0150 Acc: 0.8887 lr:0.000020
Train Epoch: 2 [2/4] Loss: 0.0252 Acc: 0.8869 lr:0.000020
Train Epoch: 2 [2/4] Loss: 0.0195 Acc: 0.8881 lr:0.000020
===================== testing phase =====================
Eval Epoch: 2 [2/5] Acc: 0.8865 F1 score: 0.8904
Train Epoch: 3 [3/4] Loss: 0.0222 Acc: 0.9375 lr:0.000020
Train Epoch: 3 [3/4] Loss: 0.0157 Acc: 0.9062 lr:0.000020
Train Epoch: 3 [3/4] Loss: 0.0146 Acc: 0.8958 lr:0.000020
Train Epoch: 3 [3/4] Loss: 0.0133 Acc: 0.8906 lr:0.000020
Train Epoch: 3 [3/4] Loss: 0.0126 Acc: 0.9000 lr:0.000020
Train Epoch: 3 [3/4] Loss: 0.0226 Acc: 0.8958 lr:0.000020
Train Epoch: 3 [3/4] Loss: 0.0244 Acc: 0.8839 lr:0.000020
Train Epoch: 3 [3/4] Loss: 0.0271 Acc: 0.8672 lr:0.000020
Train Epoch: 3 [3/4] Loss: 0.0072 Acc: 0.8819 lr:0.000020
Train Epoch: 3 [3/4] Loss: 0.0110 Acc: 0.8875 lr:0.000020
Train Epoch: 3 [3/4] Loss: 0.0338 Acc: 0.8864 lr:0.000020
Train Epoch: 3 [3/4] Loss: 0.0128 Acc: 0.8906 lr:0.000020
Train Epoch: 3 [3/4] Loss: 0.0109 Acc: 0.8942 lr:0.000020
Train Epoch: 3 [3/4] Loss: 0.0261 Acc: 0.8929 lr:0.000020
Train Epoch: 3 [3/4] Loss: 0.0141 Acc: 0.8958 lr:0.000020
Train Epoch: 3 [3/4] Loss: 0.0077 Acc: 0.8984 lr:0.000020
Train Epoch: 3 [3/4] Loss: 0.0388 Acc: 0.8860 lr:0.000020
Train Epoch: 3 [3/4] Loss: 0.0083 Acc: 0.8889 lr:0.000020
Train Epoch: 3 [3/4] Loss: 0.0166 Acc: 0.8914 lr:0.000020
Train Epoch: 3 [3/4] Loss: 0.0081 Acc: 0.8938 lr:0.000020
Train Epoch: 3 [3/4] Loss: 0.0152 Acc: 0.8929 lr:0.000020
Train Epoch: 3 [3/4] Loss: 0.0297 Acc: 0.8892 lr:0.000020
Train Epoch: 3 [3/4] Loss: 0.0178 Acc: 0.8913 lr:0.000020
Train Epoch: 3 [3/4] Loss: 0.0166 Acc: 0.8932 lr:0.000020
Train Epoch: 3 [3/4] Loss: 0.0085 Acc: 0.8950 lr:0.000020
Train Epoch: 3 [3/4] Loss: 0.0122 Acc: 0.8966 lr:0.000020
Train Epoch: 3 [3/4] Loss: 0.0189 Acc: 0.8935 lr:0.000020
Train Epoch: 3 [3/4] Loss: 0.0104 Acc: 0.8951 lr:0.000020
Train Epoch: 3 [3/4] Loss: 0.0015 Acc: 0.8987 lr:0.000020
Train Epoch: 3 [3/4] Loss: 0.0335 Acc: 0.8958 lr:0.000020
Train Epoch: 3 [3/4] Loss: 0.0104 Acc: 0.8972 lr:0.000020
Train Epoch: 3 [3/4] Loss: 0.0072 Acc: 0.9004 lr:0.000020
Train Epoch: 3 [3/4] Loss: 0.0105 Acc: 0.9015 lr:0.000020
Train Epoch: 3 [3/4] Loss: 0.0056 Acc: 0.9026 lr:0.000020
Train Epoch: 3 [3/4] Loss: 0.0120 Acc: 0.9054 lr:0.000020
Train Epoch: 3 [3/4] Loss: 0.0072 Acc: 0.9062 lr:0.000020
Train Epoch: 3 [3/4] Loss: 0.0179 Acc: 0.9054 lr:0.000020
Train Epoch: 3 [3/4] Loss: 0.0329 Acc: 0.9013 lr:0.000020
Train Epoch: 3 [3/4] Loss: 0.0134 Acc: 0.9006 lr:0.000020
Train Epoch: 3 [3/4] Loss: 0.0087 Acc: 0.9031 lr:0.000020
Train Epoch: 3 [3/4] Loss: 0.0065 Acc: 0.9040 lr:0.000020
Train Epoch: 3 [3/4] Loss: 0.0172 Acc: 0.9033 lr:0.000020
Train Epoch: 3 [3/4] Loss: 0.0058 Acc: 0.9055 lr:0.000020
===================== testing phase =====================
Eval Epoch: 3 [3/5] Acc: 0.8842 F1 score: 0.8874
Train Epoch: 4 [4/4] Loss: 0.0119 Acc: 0.9375 lr:0.000010
Train Epoch: 4 [4/4] Loss: 0.0148 Acc: 0.9375 lr:0.000010
Train Epoch: 4 [4/4] Loss: 0.0122 Acc: 0.9167 lr:0.000010
Train Epoch: 4 [4/4] Loss: 0.0058 Acc: 0.9375 lr:0.000010
Train Epoch: 4 [4/4] Loss: 0.0066 Acc: 0.9375 lr:0.000010
Train Epoch: 4 [4/4] Loss: 0.0109 Acc: 0.9375 lr:0.000010
Train Epoch: 4 [4/4] Loss: 0.0084 Acc: 0.9375 lr:0.000010
Train Epoch: 4 [4/4] Loss: 0.0243 Acc: 0.9219 lr:0.000010
Train Epoch: 4 [4/4] Loss: 0.0158 Acc: 0.9097 lr:0.000010
Train Epoch: 4 [4/4] Loss: 0.0159 Acc: 0.9062 lr:0.000010
Train Epoch: 4 [4/4] Loss: 0.0275 Acc: 0.8977 lr:0.000010
Train Epoch: 4 [4/4] Loss: 0.0053 Acc: 0.9010 lr:0.000010
Train Epoch: 4 [4/4] Loss: 0.0145 Acc: 0.8990 lr:0.000010
Train Epoch: 4 [4/4] Loss: 0.0135 Acc: 0.9018 lr:0.000010
Train Epoch: 4 [4/4] Loss: 0.0138 Acc: 0.9042 lr:0.000010
Train Epoch: 4 [4/4] Loss: 0.0090 Acc: 0.9062 lr:0.000010
Train Epoch: 4 [4/4] Loss: 0.0308 Acc: 0.9007 lr:0.000010
Train Epoch: 4 [4/4] Loss: 0.0073 Acc: 0.9028 lr:0.000010
Train Epoch: 4 [4/4] Loss: 0.0219 Acc: 0.8980 lr:0.000010
Train Epoch: 4 [4/4] Loss: 0.0171 Acc: 0.8969 lr:0.000010
Train Epoch: 4 [4/4] Loss: 0.0099 Acc: 0.8988 lr:0.000010
Train Epoch: 4 [4/4] Loss: 0.0085 Acc: 0.9006 lr:0.000010
Train Epoch: 4 [4/4] Loss: 0.0344 Acc: 0.8967 lr:0.000010
Train Epoch: 4 [4/4] Loss: 0.0379 Acc: 0.8932 lr:0.000010
Train Epoch: 4 [4/4] Loss: 0.0088 Acc: 0.8925 lr:0.000010
Train Epoch: 4 [4/4] Loss: 0.0192 Acc: 0.8918 lr:0.000010
Train Epoch: 4 [4/4] Loss: 0.0319 Acc: 0.8912 lr:0.000010
Train Epoch: 4 [4/4] Loss: 0.0131 Acc: 0.8929 lr:0.000010
Train Epoch: 4 [4/4] Loss: 0.0014 Acc: 0.8966 lr:0.000010
Train Epoch: 4 [4/4] Loss: 0.0356 Acc: 0.8917 lr:0.000010
Train Epoch: 4 [4/4] Loss: 0.0062 Acc: 0.8931 lr:0.000010
Train Epoch: 4 [4/4] Loss: 0.0033 Acc: 0.8965 lr:0.000010
Train Epoch: 4 [4/4] Loss: 0.0016 Acc: 0.8996 lr:0.000010
Train Epoch: 4 [4/4] Loss: 0.0058 Acc: 0.9026 lr:0.000010
Train Epoch: 4 [4/4] Loss: 0.0053 Acc: 0.9054 lr:0.000010
Train Epoch: 4 [4/4] Loss: 0.0033 Acc: 0.9080 lr:0.000010
Train Epoch: 4 [4/4] Loss: 0.0173 Acc: 0.9088 lr:0.000010
Train Epoch: 4 [4/4] Loss: 0.0121 Acc: 0.9062 lr:0.000010
Train Epoch: 4 [4/4] Loss: 0.0076 Acc: 0.9071 lr:0.000010
Train Epoch: 4 [4/4] Loss: 0.0063 Acc: 0.9078 lr:0.000010
Train Epoch: 4 [4/4] Loss: 0.0048 Acc: 0.9085 lr:0.000010
Train Epoch: 4 [4/4] Loss: 0.0260 Acc: 0.9077 lr:0.000010
Train Epoch: 4 [4/4] Loss: 0.0172 Acc: 0.9084 lr:0.000010
===================== testing phase =====================
Eval Epoch: 4 [4/5] Acc: 0.9014 F1 score: 0.9036
start time: 2024_07_06_14_43_15
===================== testing phase =====================
Test result: Acc: 90.1376% F1 score: 0.903587
finish time: 2024_07_06_14_43_54
