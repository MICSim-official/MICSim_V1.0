load config from['./Accuracy/config/transformer/config_stt2_bert_q8bert_train.ini']
===============================configurations===============================
Section Network   
	 model                : BERT
	 dataset              : GLUE
	 data_path            : /home/wangcong/dataset/glue_data
	 task                 : SST-2
	 pretrained           : True
	 model_path           : /home/wangcong/pretrain/huggingface/hub/models--bert-base-uncased/snapshots/models
Section Quantization
	 mode                 : Q8BERT
	 weightprecision      : 4
	 inputprecision       : 4
	 biasprecision        : 32
	 errorprecision       : -1
	 gradientprecision    : -1
	 weightsignmapping    : TwosComp
	 inputsignmapping     : TwosComp
	 weightmapping        : Sign
	 inputmapping         : Sign
	 hardware             : False
	 dumpaveragevalue     : False
	 dumpaveragevalue_path : ./log/transformer/SST2/average/
Section Training  
	 batch_size           : 32
	 max_seq_length       : 128
	 learning_rate        : 2e-5
	 bn_learning_rate     : 0.1
	 numepoch             : 12
	 decreasing_lr        : 9
	 train_log_interval   : 100
	 val_log_interval     : 1
Section Inference 
	 finetuned            : False
	 finetuned_model      : /home/wangcong/pretrain/huggingface/hub/models--bert-base-uncased/snapshots/models/SST-2/Q8BERT/test
Section Path      
	 log_dir              : ./log/transformer
	 organize             : Network_model,Network_dataset,Network_task,Quantization_mode
	 tag                  : datadump
Section CIM       
	 arraysize            : 128
	 cellprecision        : 1
	 cycleprecision       : 1
	 digitref2            : True
	 digitref3            : True
	 withcellvar          : False
Section Device    
	 resmap               : ./cell_files/fake_device.csv
	 gmincancel           : False
Section Debug     
	 printlinearerr       : False
	 printmatmulerr       : False
Section ADC       
	 mode                 : Linear
	 type                 : SAR
	 share                : 8
	 dumpdata             : False
	 dumpdatapath         : dumped_data
	 std_file             : ./ADC_files/Linear_std/test.csv
	 ref_file             : .ADC_files/NLinear_levels/test.csv
	 linear_file          : ./ADC_files/Linear/VGG8/basic/bit9.csv
	 nlinear_file         : ./ADC_files/NLinear_levels/test.csv
	 nlineartype          : KMEANS
Section DMVMCIM   
	 mixedsignal          : True
	 arraysize            : 128
	 cellprecision        : 1
	 cycleprecision       : 1
	 digitref2            : True
	 digitref3            : True
	 withcellvar          : False
Section DMVMDevice
	 resmap               : ./cimsim/Component/cell_files/fake_device_test.csv
	 gmincancel           : False
Section DMVMADC   
	 mode                 : Linear
	 type                 : SAR
	 share                : 8
	 dumpdata             : False
	 dumpdatapath         : dumped_data
	 std_file             : ./cimsim/Component/ADC_files/Linear_std/test.csv
	 ref_file             : ./cimsim/Component/ADC_files/NLinear_levels/test.csv
	 linear_file          : ./cimsim/Component/ADC_files/Linear/BERT/bit7.csv
	 nlinear_file         : ./cimsim/Component/ADC_files/NLinear_levels/test.csv
	 nlineartype          : KMEANS
Section NonIdeal  
	 noiseloc             : None
	 noisetype            : Gaussian
	 noisestd             : 10
	 printstat            : False
	 weightnoise          : 0.0
===============================configurations===============================
BertForSequenceClassification(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): Q8BERTQuantizer()
              )
              (key): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): Q8BERTQuantizer()
              )
              (value): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): Q8BERTQuantizer()
              )
              (query_activation): Q8BERTQuantizer()
              (key_activation): Q8BERTQuantizer()
              (value_activation): Q8BERTQuantizer()
              (prob_activation): Q8BERTQuantizer()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): QLinear(
                in_features=768, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
                (quantizer): Q8BERTQuantizer()
              )
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): QLinear(
              in_features=768, out_features=3072 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
              (quantizer): Q8BERTQuantizer()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): QLinear(
              in_features=3072, out_features=768 bias=True, quantize_weight=True, quantize_input=True, quantize_error=False
              (quantizer): Q8BERTQuantizer()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=2, bias=True)
)
Train Epoch: 0 [0/11] Loss: 0.0216 Acc: 0.4375 lr:0.000020
Train Epoch: 0 [0/11] Loss: 0.0215 Acc: 0.4688 lr:0.000020
Train Epoch: 0 [0/11] Loss: 0.0206 Acc: 0.5417 lr:0.000020
Train Epoch: 0 [0/11] Loss: 0.0220 Acc: 0.5156 lr:0.000020
Train Epoch: 0 [0/11] Loss: 0.0219 Acc: 0.5062 lr:0.000020
Train Epoch: 0 [0/11] Loss: 0.0221 Acc: 0.5000 lr:0.000020
Train Epoch: 0 [0/11] Loss: 0.0187 Acc: 0.5223 lr:0.000020
Train Epoch: 0 [0/11] Loss: 0.0213 Acc: 0.5273 lr:0.000020
Train Epoch: 0 [0/11] Loss: 0.0220 Acc: 0.5278 lr:0.000020
Train Epoch: 0 [0/11] Loss: 0.0172 Acc: 0.5437 lr:0.000020
Train Epoch: 0 [0/11] Loss: 0.0216 Acc: 0.5511 lr:0.000020
Train Epoch: 0 [0/11] Loss: 0.0165 Acc: 0.5625 lr:0.000020
Train Epoch: 0 [0/11] Loss: 0.0194 Acc: 0.5673 lr:0.000020
Train Epoch: 0 [0/11] Loss: 0.0196 Acc: 0.5737 lr:0.000020
Train Epoch: 0 [0/11] Loss: 0.0156 Acc: 0.5833 lr:0.000020
Train Epoch: 0 [0/11] Loss: 0.0193 Acc: 0.5938 lr:0.000020
Train Epoch: 0 [0/11] Loss: 0.0158 Acc: 0.6066 lr:0.000020
Train Epoch: 0 [0/11] Loss: 0.0165 Acc: 0.6163 lr:0.000020
Train Epoch: 0 [0/11] Loss: 0.0233 Acc: 0.6102 lr:0.000020
Train Epoch: 0 [0/11] Loss: 0.0157 Acc: 0.6156 lr:0.000020
Train Epoch: 0 [0/11] Loss: 0.0158 Acc: 0.6250 lr:0.000020
Train Epoch: 0 [0/11] Loss: 0.0212 Acc: 0.6222 lr:0.000020
===================== testing phase =====================
Eval Epoch: 0 [0/12] Acc: 0.6594 F1 score: 0.6966
save best performance model to 
/home/wangcong/pretrain/huggingface/hub/models--bert-base-uncased/snapshots/models/SST-2/Q8BERT/4bit/
Train Epoch: 1 [1/11] Loss: 0.0178 Acc: 0.8125 lr:0.000020
Train Epoch: 1 [1/11] Loss: 0.0202 Acc: 0.7188 lr:0.000020
Train Epoch: 1 [1/11] Loss: 0.0137 Acc: 0.7292 lr:0.000020
Train Epoch: 1 [1/11] Loss: 0.0156 Acc: 0.7500 lr:0.000020
Train Epoch: 1 [1/11] Loss: 0.0213 Acc: 0.7250 lr:0.000020
Train Epoch: 1 [1/11] Loss: 0.0206 Acc: 0.7083 lr:0.000020
Train Epoch: 1 [1/11] Loss: 0.0171 Acc: 0.7143 lr:0.000020
Train Epoch: 1 [1/11] Loss: 0.0213 Acc: 0.6914 lr:0.000020
Train Epoch: 1 [1/11] Loss: 0.0137 Acc: 0.7118 lr:0.000020
Train Epoch: 1 [1/11] Loss: 0.0175 Acc: 0.7156 lr:0.000020
Train Epoch: 1 [1/11] Loss: 0.0185 Acc: 0.7102 lr:0.000020
Train Epoch: 1 [1/11] Loss: 0.0140 Acc: 0.7135 lr:0.000020
Train Epoch: 1 [1/11] Loss: 0.0164 Acc: 0.7115 lr:0.000020
Train Epoch: 1 [1/11] Loss: 0.0213 Acc: 0.7143 lr:0.000020
Train Epoch: 1 [1/11] Loss: 0.0134 Acc: 0.7208 lr:0.000020
Train Epoch: 1 [1/11] Loss: 0.0121 Acc: 0.7246 lr:0.000020
Train Epoch: 1 [1/11] Loss: 0.0135 Acc: 0.7316 lr:0.000020
Train Epoch: 1 [1/11] Loss: 0.0130 Acc: 0.7378 lr:0.000020
Train Epoch: 1 [1/11] Loss: 0.0211 Acc: 0.7336 lr:0.000020
Train Epoch: 1 [1/11] Loss: 0.0130 Acc: 0.7375 lr:0.000020
Train Epoch: 1 [1/11] Loss: 0.0124 Acc: 0.7426 lr:0.000020
Train Epoch: 1 [1/11] Loss: 0.0134 Acc: 0.7472 lr:0.000020
===================== testing phase =====================
Eval Epoch: 1 [1/12] Acc: 0.7374 F1 score: 0.7359
save best performance model to 
/home/wangcong/pretrain/huggingface/hub/models--bert-base-uncased/snapshots/models/SST-2/Q8BERT/4bit/
Train Epoch: 2 [2/11] Loss: 0.0126 Acc: 0.8750 lr:0.000020
Train Epoch: 2 [2/11] Loss: 0.0190 Acc: 0.7656 lr:0.000020
Train Epoch: 2 [2/11] Loss: 0.0097 Acc: 0.8125 lr:0.000020
Train Epoch: 2 [2/11] Loss: 0.0166 Acc: 0.8047 lr:0.000020
Train Epoch: 2 [2/11] Loss: 0.0185 Acc: 0.7812 lr:0.000020
Train Epoch: 2 [2/11] Loss: 0.0215 Acc: 0.7604 lr:0.000020
Train Epoch: 2 [2/11] Loss: 0.0145 Acc: 0.7589 lr:0.000020
Train Epoch: 2 [2/11] Loss: 0.0198 Acc: 0.7383 lr:0.000020
Train Epoch: 2 [2/11] Loss: 0.0104 Acc: 0.7535 lr:0.000020
Train Epoch: 2 [2/11] Loss: 0.0143 Acc: 0.7531 lr:0.000020
Train Epoch: 2 [2/11] Loss: 0.0134 Acc: 0.7557 lr:0.000020
Train Epoch: 2 [2/11] Loss: 0.0086 Acc: 0.7682 lr:0.000020
Train Epoch: 2 [2/11] Loss: 0.0136 Acc: 0.7716 lr:0.000020
Train Epoch: 2 [2/11] Loss: 0.0185 Acc: 0.7679 lr:0.000020
Train Epoch: 2 [2/11] Loss: 0.0120 Acc: 0.7750 lr:0.000020
Train Epoch: 2 [2/11] Loss: 0.0120 Acc: 0.7773 lr:0.000020
Train Epoch: 2 [2/11] Loss: 0.0142 Acc: 0.7757 lr:0.000020
Train Epoch: 2 [2/11] Loss: 0.0125 Acc: 0.7812 lr:0.000020
Train Epoch: 2 [2/11] Loss: 0.0198 Acc: 0.7730 lr:0.000020
Train Epoch: 2 [2/11] Loss: 0.0128 Acc: 0.7766 lr:0.000020
Train Epoch: 2 [2/11] Loss: 0.0110 Acc: 0.7827 lr:0.000020
Train Epoch: 2 [2/11] Loss: 0.0125 Acc: 0.7841 lr:0.000020
===================== testing phase =====================
Eval Epoch: 2 [2/12] Acc: 0.7580 F1 score: 0.7605
save best performance model to 
/home/wangcong/pretrain/huggingface/hub/models--bert-base-uncased/snapshots/models/SST-2/Q8BERT/4bit/
Train Epoch: 3 [3/11] Loss: 0.0117 Acc: 0.8438 lr:0.000020
Train Epoch: 3 [3/11] Loss: 0.0151 Acc: 0.7969 lr:0.000020
Train Epoch: 3 [3/11] Loss: 0.0107 Acc: 0.7917 lr:0.000020
Train Epoch: 3 [3/11] Loss: 0.0110 Acc: 0.7969 lr:0.000020
Train Epoch: 3 [3/11] Loss: 0.0173 Acc: 0.7937 lr:0.000020
Train Epoch: 3 [3/11] Loss: 0.0196 Acc: 0.7760 lr:0.000020
Train Epoch: 3 [3/11] Loss: 0.0106 Acc: 0.7902 lr:0.000020
Train Epoch: 3 [3/11] Loss: 0.0193 Acc: 0.7773 lr:0.000020
Train Epoch: 3 [3/11] Loss: 0.0111 Acc: 0.7778 lr:0.000020
Train Epoch: 3 [3/11] Loss: 0.0144 Acc: 0.7688 lr:0.000020
Train Epoch: 3 [3/11] Loss: 0.0186 Acc: 0.7642 lr:0.000020
Train Epoch: 3 [3/11] Loss: 0.0148 Acc: 0.7682 lr:0.000020
Train Epoch: 3 [3/11] Loss: 0.0140 Acc: 0.7716 lr:0.000020
Train Epoch: 3 [3/11] Loss: 0.0120 Acc: 0.7768 lr:0.000020
Train Epoch: 3 [3/11] Loss: 0.0081 Acc: 0.7875 lr:0.000020
Train Epoch: 3 [3/11] Loss: 0.0082 Acc: 0.7949 lr:0.000020
Train Epoch: 3 [3/11] Loss: 0.0109 Acc: 0.7978 lr:0.000020
Train Epoch: 3 [3/11] Loss: 0.0121 Acc: 0.7986 lr:0.000020
Train Epoch: 3 [3/11] Loss: 0.0154 Acc: 0.7961 lr:0.000020
Train Epoch: 3 [3/11] Loss: 0.0100 Acc: 0.7984 lr:0.000020
Train Epoch: 3 [3/11] Loss: 0.0106 Acc: 0.8021 lr:0.000020
Train Epoch: 3 [3/11] Loss: 0.0114 Acc: 0.8026 lr:0.000020
===================== testing phase =====================
Eval Epoch: 3 [3/12] Acc: 0.7683 F1 score: 0.7678
save best performance model to 
/home/wangcong/pretrain/huggingface/hub/models--bert-base-uncased/snapshots/models/SST-2/Q8BERT/4bit/
Train Epoch: 4 [4/11] Loss: 0.0085 Acc: 0.9062 lr:0.000020
Train Epoch: 4 [4/11] Loss: 0.0185 Acc: 0.8281 lr:0.000020
Train Epoch: 4 [4/11] Loss: 0.0067 Acc: 0.8542 lr:0.000020
Train Epoch: 4 [4/11] Loss: 0.0096 Acc: 0.8594 lr:0.000020
Train Epoch: 4 [4/11] Loss: 0.0183 Acc: 0.8250 lr:0.000020
Train Epoch: 4 [4/11] Loss: 0.0151 Acc: 0.8125 lr:0.000020
Train Epoch: 4 [4/11] Loss: 0.0110 Acc: 0.8170 lr:0.000020
Train Epoch: 4 [4/11] Loss: 0.0178 Acc: 0.7969 lr:0.000020
Train Epoch: 4 [4/11] Loss: 0.0116 Acc: 0.7986 lr:0.000020
Train Epoch: 4 [4/11] Loss: 0.0131 Acc: 0.8000 lr:0.000020
Train Epoch: 4 [4/11] Loss: 0.0139 Acc: 0.8040 lr:0.000020
Train Epoch: 4 [4/11] Loss: 0.0114 Acc: 0.8073 lr:0.000020
Train Epoch: 4 [4/11] Loss: 0.0077 Acc: 0.8149 lr:0.000020
Train Epoch: 4 [4/11] Loss: 0.0130 Acc: 0.8103 lr:0.000020
Train Epoch: 4 [4/11] Loss: 0.0070 Acc: 0.8167 lr:0.000020
Train Epoch: 4 [4/11] Loss: 0.0057 Acc: 0.8223 lr:0.000020
Train Epoch: 4 [4/11] Loss: 0.0097 Acc: 0.8235 lr:0.000020
Train Epoch: 4 [4/11] Loss: 0.0104 Acc: 0.8264 lr:0.000020
Train Epoch: 4 [4/11] Loss: 0.0178 Acc: 0.8207 lr:0.000020
Train Epoch: 4 [4/11] Loss: 0.0100 Acc: 0.8203 lr:0.000020
Train Epoch: 4 [4/11] Loss: 0.0081 Acc: 0.8259 lr:0.000020
Train Epoch: 4 [4/11] Loss: 0.0113 Acc: 0.8253 lr:0.000020
===================== testing phase =====================
Eval Epoch: 4 [4/12] Acc: 0.7672 F1 score: 0.7805
Train Epoch: 5 [5/11] Loss: 0.0064 Acc: 0.9062 lr:0.000020
Train Epoch: 5 [5/11] Loss: 0.0160 Acc: 0.8438 lr:0.000020
Train Epoch: 5 [5/11] Loss: 0.0048 Acc: 0.8854 lr:0.000020
Train Epoch: 5 [5/11] Loss: 0.0078 Acc: 0.8828 lr:0.000020
Train Epoch: 5 [5/11] Loss: 0.0136 Acc: 0.8750 lr:0.000020
Train Epoch: 5 [5/11] Loss: 0.0163 Acc: 0.8438 lr:0.000020
Train Epoch: 5 [5/11] Loss: 0.0073 Acc: 0.8527 lr:0.000020
Train Epoch: 5 [5/11] Loss: 0.0120 Acc: 0.8438 lr:0.000020
Train Epoch: 5 [5/11] Loss: 0.0088 Acc: 0.8472 lr:0.000020
Train Epoch: 5 [5/11] Loss: 0.0128 Acc: 0.8344 lr:0.000020
Train Epoch: 5 [5/11] Loss: 0.0086 Acc: 0.8409 lr:0.000020
Train Epoch: 5 [5/11] Loss: 0.0119 Acc: 0.8411 lr:0.000020
Train Epoch: 5 [5/11] Loss: 0.0106 Acc: 0.8413 lr:0.000020
Train Epoch: 5 [5/11] Loss: 0.0115 Acc: 0.8438 lr:0.000020
Train Epoch: 5 [5/11] Loss: 0.0062 Acc: 0.8521 lr:0.000020
Train Epoch: 5 [5/11] Loss: 0.0057 Acc: 0.8594 lr:0.000020
Train Epoch: 5 [5/11] Loss: 0.0082 Acc: 0.8603 lr:0.000020
Train Epoch: 5 [5/11] Loss: 0.0122 Acc: 0.8594 lr:0.000020
Train Epoch: 5 [5/11] Loss: 0.0139 Acc: 0.8569 lr:0.000020
Train Epoch: 5 [5/11] Loss: 0.0052 Acc: 0.8625 lr:0.000020
Train Epoch: 5 [5/11] Loss: 0.0071 Acc: 0.8646 lr:0.000020
Train Epoch: 5 [5/11] Loss: 0.0072 Acc: 0.8665 lr:0.000020
===================== testing phase =====================
Eval Epoch: 5 [5/12] Acc: 0.7947 F1 score: 0.7987
save best performance model to 
/home/wangcong/pretrain/huggingface/hub/models--bert-base-uncased/snapshots/models/SST-2/Q8BERT/4bit/
Train Epoch: 6 [6/11] Loss: 0.0083 Acc: 0.9062 lr:0.000020
Train Epoch: 6 [6/11] Loss: 0.0179 Acc: 0.8594 lr:0.000020
Train Epoch: 6 [6/11] Loss: 0.0040 Acc: 0.8958 lr:0.000020
Train Epoch: 6 [6/11] Loss: 0.0096 Acc: 0.8828 lr:0.000020
Train Epoch: 6 [6/11] Loss: 0.0157 Acc: 0.8625 lr:0.000020
Train Epoch: 6 [6/11] Loss: 0.0122 Acc: 0.8490 lr:0.000020
Train Epoch: 6 [6/11] Loss: 0.0086 Acc: 0.8482 lr:0.000020
Train Epoch: 6 [6/11] Loss: 0.0139 Acc: 0.8398 lr:0.000020
Train Epoch: 6 [6/11] Loss: 0.0083 Acc: 0.8438 lr:0.000020
Train Epoch: 6 [6/11] Loss: 0.0115 Acc: 0.8438 lr:0.000020
Train Epoch: 6 [6/11] Loss: 0.0105 Acc: 0.8494 lr:0.000020
Train Epoch: 6 [6/11] Loss: 0.0097 Acc: 0.8490 lr:0.000020
Train Epoch: 6 [6/11] Loss: 0.0049 Acc: 0.8582 lr:0.000020
Train Epoch: 6 [6/11] Loss: 0.0106 Acc: 0.8594 lr:0.000020
Train Epoch: 6 [6/11] Loss: 0.0074 Acc: 0.8604 lr:0.000020
Train Epoch: 6 [6/11] Loss: 0.0057 Acc: 0.8652 lr:0.000020
Train Epoch: 6 [6/11] Loss: 0.0071 Acc: 0.8676 lr:0.000020
Train Epoch: 6 [6/11] Loss: 0.0079 Acc: 0.8663 lr:0.000020
Train Epoch: 6 [6/11] Loss: 0.0121 Acc: 0.8618 lr:0.000020
Train Epoch: 6 [6/11] Loss: 0.0043 Acc: 0.8656 lr:0.000020
Train Epoch: 6 [6/11] Loss: 0.0074 Acc: 0.8690 lr:0.000020
Train Epoch: 6 [6/11] Loss: 0.0089 Acc: 0.8693 lr:0.000020
===================== testing phase =====================
Eval Epoch: 6 [6/12] Acc: 0.7982 F1 score: 0.8049
save best performance model to 
/home/wangcong/pretrain/huggingface/hub/models--bert-base-uncased/snapshots/models/SST-2/Q8BERT/4bit/
Train Epoch: 7 [7/11] Loss: 0.0039 Acc: 0.9688 lr:0.000020
Train Epoch: 7 [7/11] Loss: 0.0146 Acc: 0.8906 lr:0.000020
Train Epoch: 7 [7/11] Loss: 0.0045 Acc: 0.9062 lr:0.000020
Train Epoch: 7 [7/11] Loss: 0.0085 Acc: 0.8984 lr:0.000020
Train Epoch: 7 [7/11] Loss: 0.0171 Acc: 0.8812 lr:0.000020
Train Epoch: 7 [7/11] Loss: 0.0116 Acc: 0.8750 lr:0.000020
Train Epoch: 7 [7/11] Loss: 0.0090 Acc: 0.8750 lr:0.000020
Train Epoch: 7 [7/11] Loss: 0.0113 Acc: 0.8789 lr:0.000020
Train Epoch: 7 [7/11] Loss: 0.0106 Acc: 0.8819 lr:0.000020
Train Epoch: 7 [7/11] Loss: 0.0080 Acc: 0.8844 lr:0.000020
Train Epoch: 7 [7/11] Loss: 0.0160 Acc: 0.8750 lr:0.000020
Train Epoch: 7 [7/11] Loss: 0.0124 Acc: 0.8724 lr:0.000020
Train Epoch: 7 [7/11] Loss: 0.0059 Acc: 0.8798 lr:0.000020
Train Epoch: 7 [7/11] Loss: 0.0068 Acc: 0.8817 lr:0.000020
Train Epoch: 7 [7/11] Loss: 0.0052 Acc: 0.8875 lr:0.000020
Train Epoch: 7 [7/11] Loss: 0.0035 Acc: 0.8926 lr:0.000020
Train Epoch: 7 [7/11] Loss: 0.0063 Acc: 0.8934 lr:0.000020
Train Epoch: 7 [7/11] Loss: 0.0054 Acc: 0.8958 lr:0.000020
Train Epoch: 7 [7/11] Loss: 0.0096 Acc: 0.8947 lr:0.000020
Train Epoch: 7 [7/11] Loss: 0.0056 Acc: 0.8969 lr:0.000020
Train Epoch: 7 [7/11] Loss: 0.0057 Acc: 0.9003 lr:0.000020
Train Epoch: 7 [7/11] Loss: 0.0072 Acc: 0.9020 lr:0.000020
===================== testing phase =====================
Eval Epoch: 7 [7/12] Acc: 0.8154 F1 score: 0.8193
save best performance model to 
/home/wangcong/pretrain/huggingface/hub/models--bert-base-uncased/snapshots/models/SST-2/Q8BERT/4bit/
Train Epoch: 8 [8/11] Loss: 0.0031 Acc: 0.9688 lr:0.000020
Train Epoch: 8 [8/11] Loss: 0.0124 Acc: 0.9062 lr:0.000020
Train Epoch: 8 [8/11] Loss: 0.0047 Acc: 0.9062 lr:0.000020
